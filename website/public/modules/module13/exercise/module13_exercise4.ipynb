{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/module13/exercise/module13_exercise4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uVgWUZjpb137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "outputId": "b309087b-5b1b-4d5c-d454-5c50a2e1629a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig==4.2.1 in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gymnasium"
                ]
              },
              "id": "dd242749158e43cea5f793ba1ed5597f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gymnasium[box2d]  # Install Box2D dependency for LunarLander-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module13_exercise4 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/1\" target=\"_blank\"> LunarLander</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/1\" target=\"_blank\"> LunarLander</a> with mean reward upper than 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lCv3K7-8S-K"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLDC1kaM8S-K",
        "outputId": "cd316cfc-44d8-44d4-da77-ea3195b2404a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Épisode 10/1000 - Score moyen (100 derniers): -235.8 - eps=0.951\n",
            "Épisode 20/1000 - Score moyen (100 derniers): -210.6 - eps=0.905\n",
            "Épisode 30/1000 - Score moyen (100 derniers): -211.2 - eps=0.860\n",
            "Épisode 40/1000 - Score moyen (100 derniers): -201.9 - eps=0.818\n",
            "Épisode 50/1000 - Score moyen (100 derniers): -204.5 - eps=0.778\n",
            "Épisode 60/1000 - Score moyen (100 derniers): -202.1 - eps=0.740\n",
            "Épisode 70/1000 - Score moyen (100 derniers): -192.8 - eps=0.704\n",
            "Épisode 80/1000 - Score moyen (100 derniers): -189.7 - eps=0.670\n",
            "Épisode 90/1000 - Score moyen (100 derniers): -181.9 - eps=0.637\n",
            "Épisode 100/1000 - Score moyen (100 derniers): -177.8 - eps=0.606\n",
            "Épisode 110/1000 - Score moyen (100 derniers): -167.6 - eps=0.576\n",
            "Épisode 120/1000 - Score moyen (100 derniers): -163.2 - eps=0.548\n",
            "Épisode 130/1000 - Score moyen (100 derniers): -154.8 - eps=0.521\n",
            "Épisode 140/1000 - Score moyen (100 derniers): -148.5 - eps=0.496\n",
            "Épisode 150/1000 - Score moyen (100 derniers): -137.2 - eps=0.471\n",
            "Épisode 160/1000 - Score moyen (100 derniers): -125.0 - eps=0.448\n",
            "Épisode 170/1000 - Score moyen (100 derniers): -117.7 - eps=0.427\n",
            "Épisode 180/1000 - Score moyen (100 derniers): -109.0 - eps=0.406\n",
            "Épisode 190/1000 - Score moyen (100 derniers): -108.5 - eps=0.386\n",
            "Épisode 200/1000 - Score moyen (100 derniers): -103.7 - eps=0.367\n",
            "Épisode 210/1000 - Score moyen (100 derniers): -93.8 - eps=0.349\n",
            "Épisode 220/1000 - Score moyen (100 derniers): -83.6 - eps=0.332\n",
            "Épisode 230/1000 - Score moyen (100 derniers): -73.7 - eps=0.316\n",
            "Épisode 240/1000 - Score moyen (100 derniers): -66.4 - eps=0.300\n",
            "Épisode 250/1000 - Score moyen (100 derniers): -68.3 - eps=0.286\n",
            "Épisode 260/1000 - Score moyen (100 derniers): -68.9 - eps=0.272\n",
            "Épisode 270/1000 - Score moyen (100 derniers): -67.2 - eps=0.258\n",
            "Épisode 280/1000 - Score moyen (100 derniers): -62.0 - eps=0.246\n",
            "Épisode 290/1000 - Score moyen (100 derniers): -52.7 - eps=0.234\n",
            "Épisode 300/1000 - Score moyen (100 derniers): -45.2 - eps=0.222\n",
            "Épisode 310/1000 - Score moyen (100 derniers): -45.1 - eps=0.211\n",
            "Épisode 320/1000 - Score moyen (100 derniers): -42.4 - eps=0.201\n",
            "Épisode 330/1000 - Score moyen (100 derniers): -41.8 - eps=0.191\n",
            "Épisode 340/1000 - Score moyen (100 derniers): -39.7 - eps=0.182\n",
            "Épisode 350/1000 - Score moyen (100 derniers): -28.6 - eps=0.173\n",
            "Épisode 360/1000 - Score moyen (100 derniers): -22.8 - eps=0.165\n",
            "Épisode 370/1000 - Score moyen (100 derniers): -19.7 - eps=0.157\n",
            "Épisode 380/1000 - Score moyen (100 derniers): -13.0 - eps=0.149\n",
            "Épisode 390/1000 - Score moyen (100 derniers): -12.7 - eps=0.142\n",
            "Épisode 400/1000 - Score moyen (100 derniers): -7.1 - eps=0.135\n",
            "Épisode 410/1000 - Score moyen (100 derniers): 4.6 - eps=0.128\n",
            "Épisode 420/1000 - Score moyen (100 derniers): 14.4 - eps=0.122\n",
            "Épisode 430/1000 - Score moyen (100 derniers): 20.3 - eps=0.116\n",
            "Épisode 440/1000 - Score moyen (100 derniers): 26.2 - eps=0.110\n",
            "Épisode 450/1000 - Score moyen (100 derniers): 45.2 - eps=0.105\n",
            "Épisode 460/1000 - Score moyen (100 derniers): 54.6 - eps=0.100\n",
            "Épisode 470/1000 - Score moyen (100 derniers): 65.1 - eps=0.095\n",
            "Épisode 480/1000 - Score moyen (100 derniers): 66.6 - eps=0.090\n",
            "Épisode 490/1000 - Score moyen (100 derniers): 78.0 - eps=0.086\n",
            "Épisode 500/1000 - Score moyen (100 derniers): 82.2 - eps=0.082\n",
            "Épisode 510/1000 - Score moyen (100 derniers): 86.1 - eps=0.078\n",
            "Épisode 520/1000 - Score moyen (100 derniers): 93.3 - eps=0.074\n",
            "Épisode 530/1000 - Score moyen (100 derniers): 105.1 - eps=0.070\n",
            "Épisode 540/1000 - Score moyen (100 derniers): 114.2 - eps=0.067\n",
            "Épisode 550/1000 - Score moyen (100 derniers): 117.5 - eps=0.063\n",
            "Épisode 560/1000 - Score moyen (100 derniers): 127.5 - eps=0.060\n",
            "Épisode 570/1000 - Score moyen (100 derniers): 134.0 - eps=0.057\n",
            "Épisode 580/1000 - Score moyen (100 derniers): 144.3 - eps=0.055\n",
            "Épisode 590/1000 - Score moyen (100 derniers): 148.1 - eps=0.052\n",
            "Épisode 600/1000 - Score moyen (100 derniers): 154.4 - eps=0.049\n",
            "Épisode 610/1000 - Score moyen (100 derniers): 156.6 - eps=0.047\n",
            "Épisode 620/1000 - Score moyen (100 derniers): 155.2 - eps=0.045\n",
            "Épisode 630/1000 - Score moyen (100 derniers): 156.3 - eps=0.043\n",
            "Épisode 640/1000 - Score moyen (100 derniers): 165.7 - eps=0.040\n",
            "Épisode 650/1000 - Score moyen (100 derniers): 162.0 - eps=0.038\n",
            "Épisode 660/1000 - Score moyen (100 derniers): 165.0 - eps=0.037\n",
            "Épisode 670/1000 - Score moyen (100 derniers): 172.4 - eps=0.035\n",
            "Épisode 680/1000 - Score moyen (100 derniers): 180.6 - eps=0.033\n",
            "Épisode 690/1000 - Score moyen (100 derniers): 188.0 - eps=0.031\n",
            "Épisode 700/1000 - Score moyen (100 derniers): 193.1 - eps=0.030\n",
            "Épisode 710/1000 - Score moyen (100 derniers): 196.7 - eps=0.028\n",
            "Environnement résolu en 716 épisodes 🎉  (score moyen sur 100 eps = 201.0)\n",
            "Meilleure moyenne obtenue sur 100 épisodes: 201.00271275072026\n",
            "Récompense moyenne sur 100 épisodes d'évaluation: 163.60\n",
            ">>> Performance cible NON atteinte. Réentraîner ou ajuster les hyperparamètres ⚠️\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gymnasium as gym  # nécessite gymnasium[box2d] pour LunarLander-v2\n",
        "\n",
        "# Définition du réseau de neurones pour approximer Q(s,a)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Réseau fully-connected avec 2 couches cachées de 128 neurones\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc_out = nn.Linear(128, action_dim)\n",
        "        # Initialisation optionnelle des poids peut être ajoutée ici si désiré\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Passe avant : ReLU sur couches cachées\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc_out(x)  # sorties Q-values (une par action)\n",
        "\n",
        "# Buffer d'expérience pour stocker et échantillonner des transitions\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, state_dim):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []        # liste de transitions\n",
        "        self.position = 0       # index courant pour écraser les anciennes expériences\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Si la mémoire n'est pas encore pleine, on ajoute une nouvelle entrée\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        # Stocker la transition (on copie les tableaux pour éviter les références)\n",
        "        self.memory[self.position] = (\n",
        "            np.array(state, copy=True),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, copy=True),\n",
        "            done\n",
        "        )\n",
        "        # Incrément circulaire de la position\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Tirer aléatoirement batch_size transitions\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*(self.memory[i] for i in indices))\n",
        "        # Convertir en tenseurs PyTorch\n",
        "        states      = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        actions     = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)    # actions indices\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)  # récompenses\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "        dones       = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)    # indicateurs de fin (0.0 ou 1.0)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Agent DQN avec réseau local et réseau cible\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        # Initialiser les deux réseaux (policy et target) et l'optimiseur\n",
        "        self.q_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())  # initialisation identique\n",
        "        self.target_network.eval()  # le réseau cible n'est pas entraîné par gradient\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=5e-4)\n",
        "        # Initialiser la mémoire d'expérience\n",
        "        self.memory = ReplayBuffer(capacity=100000, state_dim=state_dim)\n",
        "        # Compteur de pas pour gestion des mises à jour\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\"Renvoie une action selon une politique epsilon-greedy.\"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            # Exploration aléatoire\n",
        "            return np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            # Exploitation (on choisit l'action de Q maximale)\n",
        "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # shape (1, state_dim)\n",
        "            self.q_network.eval()  # mode évaluation\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_t)\n",
        "            self.q_network.train()  # repasse en mode entraînement\n",
        "            action = int(torch.argmax(q_values, dim=1).item())\n",
        "            return action\n",
        "\n",
        "    def train_step(self, batch_size=64, gamma=0.99, tau=1e-3):\n",
        "        \"\"\"Effectue un pas d'apprentissage du réseau (une mise à jour de Q-network).\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return  # ne pas entraîner tant qu'on n'a pas assez d'échantillons\n",
        "        # Échantillonner un mini-batch de transitions\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
        "        # Calcul des Q-cibles avec le réseau cible (on ne calcule pas de gradients ici)\n",
        "        with torch.no_grad():\n",
        "            # Valeur Q max du prochain état selon le réseau cible\n",
        "            q_next = self.target_network(next_states).max(dim=1, keepdim=True)[0]\n",
        "            # Cible de Q: r + gamma * max(Q_next) * (1 - done)\n",
        "            q_target = rewards + gamma * q_next * (1 - dones)\n",
        "        # Valeur Q courante prédite par le réseau principal pour les (state, action) du batch\n",
        "        q_current = self.q_network(states).gather(1, actions)  # Q(s,a) pour chaque transition du batch\n",
        "        # Calcul de la perte (erreur quadratique)\n",
        "        loss = F.mse_loss(q_current, q_target)\n",
        "        # Rétropropagation de la perte et mise à jour des poids du Q-network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # Mise à jour douce du réseau cible vers le Q-network (tau)\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "# Environnement LunarLander-v2\n",
        "# Environnement LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\") # Changed from v3 to v2\n",
        "state_dim = env.observation_space.shape[0]   # dimension d'état (8)\n",
        "action_dim = env.action_space.n             # nombre d'actions (4)\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# Paramètres d'entraînement\n",
        "num_episodes = 1000         # nombre maximal d'épisodes\n",
        "max_steps = 1000            # pas max par épisode (pour éviter des boucles infinies)\n",
        "target_score = 200          # score cible à atteindre en moyenne\n",
        "print_interval = 10         # intervalle pour affichage des progrès\n",
        "best_avg_reward = -float(\"inf\")\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "# Variables pour suivi de la performance\n",
        "scores = []                 # liste des scores par épisode\n",
        "scores_window = []          # fenêtre glissante des derniers 100 scores\n",
        "\n",
        "# Boucle principale d'entraînement\n",
        "epsilon = 1.0               # valeur initiale de epsilon (politique epsilon-greedy)\n",
        "epsilon_decay = 0.995       # facteur de décroissance exponentielle de epsilon\n",
        "epsilon_min = 0.01          # epsilon minimum\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    state, _ = env.reset()  # réinitialiser l'environnement\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        # Sélectionner une action selon la politique epsilon-greedy\n",
        "        action = agent.select_action(state, epsilon)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "        # Ajouter la transition dans la mémoire\n",
        "        agent.memory.add(state, action, reward, next_state, done)\n",
        "        # Mettre à jour l'état courant\n",
        "        state = next_state\n",
        "        # Entraîner le réseau (toutes les 4 étapes)\n",
        "        if t % 4 == 0:\n",
        "            agent.train_step(batch_size=64, gamma=0.99, tau=1e-3)\n",
        "        # Sortir si fin d'épisode\n",
        "        if done:\n",
        "            break\n",
        "    # Mettre à jour epsilon (décroissance exponentielle par épisode)\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "    # Enregistrer le score de l'épisode\n",
        "    scores.append(episode_reward)\n",
        "    scores_window.append(episode_reward)\n",
        "    if len(scores_window) > 100:\n",
        "        # garder une fenêtre glissante de 100 derniers épisodes\n",
        "        scores_window.pop(0)\n",
        "    # Calculer la récompense moyenne des 100 derniers épisodes\n",
        "    avg_reward_100 = np.mean(scores_window)\n",
        "    # Sauvegarder le modèle si c'est le meilleur jusqu'à présent\n",
        "    if avg_reward_100 > best_avg_reward:\n",
        "        best_avg_reward = avg_reward_100\n",
        "        torch.save(agent.q_network.state_dict(), best_model_path)\n",
        "    # Affichage périodique des statistiques d'entraînement\n",
        "    if episode % print_interval == 0:\n",
        "        print(f\"Épisode {episode}/{num_episodes} - Score moyen (100 derniers): {avg_reward_100:.1f} - eps={epsilon:.3f}\")\n",
        "    # Arrêt anticipé si la moyenne sur 100 épisodes atteint la cible\n",
        "    if avg_reward_100 >= target_score and episode >= 100:\n",
        "        print(f\"Environnement résolu en {episode} épisodes 🎉  (score moyen sur 100 eps = {avg_reward_100:.1f})\")\n",
        "        break\n",
        "\n",
        "# Fin de l'entraînement\n",
        "print(\"Meilleure moyenne obtenue sur 100 épisodes:\", best_avg_reward)\n",
        "# Charger le meilleur modèle sauvegardé\n",
        "best_model = QNetwork(state_dim, action_dim)\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "best_model.eval()\n",
        "\n",
        "# Évaluation du modèle entraîné sur 100 épisodes pour vérifier la performance > 200\n",
        "eval_episodes = 100\n",
        "eval_rewards = []\n",
        "for i in range(eval_episodes):\n",
        "    state, _ = env.reset()\n",
        "    episode_sum = 0\n",
        "    while True:\n",
        "        # Sélectionner action de façon déterministe (epsilon=0, politique purement optimisée)\n",
        "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_vals = best_model(state_t)\n",
        "        action = int(torch.argmax(q_vals, dim=1).item())\n",
        "        # Agir dans l'environnement\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_sum += reward\n",
        "        state = next_state\n",
        "        if terminated or truncated:\n",
        "            eval_rewards.append(episode_sum)\n",
        "            break\n",
        "\n",
        "avg_eval_reward = np.mean(eval_rewards)\n",
        "print(f\"Récompense moyenne sur {eval_episodes} épisodes d'évaluation: {avg_eval_reward:.2f}\")\n",
        "if avg_eval_reward >= 200:\n",
        "    print(\">>> Performance cible atteinte! L'agent obtient en moyenne au-dessus de 200 ✅\")\n",
        "else:\n",
        "    print(\">>> Performance cible NON atteinte. Réentraîner ou ajuster les hyperparamètres ⚠️\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mWGRAlg8S-L"
      },
      "source": [
        "### Description\n",
        "\n",
        "This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
        "\n",
        "### Action Space\n",
        "\n",
        "There are four discrete actions available:\n",
        "- 0: do nothing\n",
        "- 1: fire left orientation engine\n",
        "- 2: fire main engine\n",
        "- 3: fire right orientation engine\n",
        "\n",
        "### Observation Space\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "### Rewards\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "For each step, the reward:\n",
        "- is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- is increased/decreased the slower/faster the lander is moving.\n",
        "- is decreased the more the lander is tilted (angle not horizontal).\n",
        "- is increased by 10 points for each leg that is in contact with the ground.\n",
        "- is decreased by 0.03 points each frame a side engine is firing.\n",
        "- is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "\n",
        "### Starting State\n",
        "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
        "\n",
        "### Episode Termination\n",
        "The episode finishes if:\n",
        "- the lander crashes (the lander body gets in contact with the moon);\n",
        "- the lander gets outside of the viewport (x coordinate is greater than 1);\n",
        "- the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RsBDhoU8S-O"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "agent = Agent(env)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ],
      "metadata": {
        "id": "6ik3Mva0mhUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}